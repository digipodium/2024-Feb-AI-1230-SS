{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/digipodium/Datasets/main/regression/house_pricing.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature analysis and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation between area and price\n",
    "plt.scatter(df.SquareFeet, df.Price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check correlation between features and target when using linear models\n",
    "- features are numerical\n",
    "- predictors are numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(df[['SquareFeet']], df.Price)\n",
    "pred = model.predict(df[['SquareFeet']])\n",
    "plt.scatter(df.SquareFeet, df.Price)\n",
    "plt.plot(df.SquareFeet, pred, color='red')\n",
    "# pearson correlation\n",
    "pc = df.SquareFeet.corr(df.Price)\n",
    "print('Pearson Correlation:', pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include='number').corrwith(df.Price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pearson correlation coefficient\n",
    "- if value close to 1, strong positive correlation\n",
    "- if value close to -1, strong negative correlation\n",
    "- if value close to 0, no correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include='number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include='number').columns\n",
    "for col in num_cols:\n",
    "    plt.scatter(df[col], df.Price)  \n",
    "    plt.title(col)\n",
    "    model = LinearRegression()\n",
    "    model.fit(df[[col]], df.Price)\n",
    "    plt.plot(df[col], model.predict(df[[col]]), color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for categorical independent variables with numerical target\n",
    "    - boxplot analysis (visual)\n",
    "    - anova test\n",
    "        - null hypothesis: means of the groups are equal\n",
    "        - if p-value < 0.05, reject null hypothesis\n",
    "        - if p-value > 0.05, fail to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Beds', 'Baths','City','Type']\n",
    "for col in cat_cols:\n",
    "    df.boxplot(column='Price', by=col, grid=False)\n",
    "    plt.title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing with anova\n",
    "- for each feature, calculate the f-statistic and p-value\n",
    "- if p-value < 0.05, reject null hypothesis (means the column is important)\n",
    "- if p-value > 0.05, fail to reject null hypothesis (means the column is not important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['SquareFeet', 'Beds', 'Baths', 'Type']\n",
    "X = df[selected_cols]\n",
    "y = df.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = X.select_dtypes(include='number').columns\n",
    "cat_cols = X.select_dtypes(include='object').columns\n",
    "num_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipe = Pipeline([   \n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipe, num_cols),\n",
    "    ('cat', cat_pipe, cat_cols)\n",
    "])\n",
    "model = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('lr', LinearRegression())\n",
    "])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a learning curve test can be used to check if the model is overfitting or underfitting at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "print(\"Training Results\")\n",
    "print(\"MSE:\", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_train, y_train_pred))\n",
    "print(\"R2:\", r2_score(y_train, y_train_pred))\n",
    "print('-'*50)\n",
    "print(\"Testing Results\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_test_pred))\n",
    "print(\"R2:\", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "# create a directory to save models\n",
    "os.makedirs(os.path.join(os.getcwd(), 'models'), exist_ok=True)\n",
    "# save model - dumping\n",
    "path = os.path.join(os.getcwd(), 'models', 'house_price_model')\n",
    "joblib.dump(model,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), 'models', 'house_price_model')\n",
    "model = joblib.load(path)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making predictions with a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very simple example data\n",
    "print(model.predict(X.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.Type.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual data input\n",
    "inpX = pd.DataFrame({\n",
    "    'SquareFeet': [1200],\n",
    "    'Beds': [5],\n",
    "    'Baths': [5],\n",
    "    'Type': ['Residential']\n",
    "})\n",
    "result = model.predict(inpX)\n",
    "print(*result, sep='\\n') # short cut to print each element of list on new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluating the training and testing performance of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "\n",
    "train_sizes = [.1, .2, .3, .4, .5, .6, .7, .8, .9,]\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5, \n",
    "                                                        train_sizes=train_sizes)\n",
    "\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), label='Train', marker='o')\n",
    "plt.plot(train_sizes, test_scores.mean(axis=1), label='Test', marker='o')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making a decision tree model, to check the updated accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('dt', DecisionTreeRegressor())\n",
    "])\n",
    "model2.fit(X_train, y_train)\n",
    "y_train_pred = model2.predict(X_train)\n",
    "y_test_pred = model2.predict(X_test)\n",
    "\n",
    "print(\"Training Results\")\n",
    "print(\"MSE:\", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_train, y_train_pred))\n",
    "print(\"R2:\", r2_score(y_train, y_train_pred))\n",
    "print('-'*50)\n",
    "print(\"Testing Results\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_test_pred))\n",
    "print(\"R2:\", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,30))\n",
    "_ = plot_tree(\n",
    "    model2.named_steps['dt'],\n",
    "    filled=True,\n",
    "    max_depth=5,\n",
    "    fontsize=14,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
